// deno-lint-ignore-file no-explicit-any
/**
 * This module provides a Stage 3 decorator factory for caching method calls
 * using an advanced Least Recently Used (LRU) strategy with TTL support.
 *
 * ## Background
 *
 * Modeled after Pythonâ€™s `@lru_cache` decorator, this decorator caches the
 * target method's return values based on the arguments it receives.
 *
 * ## Caching
 *
 * The core caching logic lives in a dedicated {@link LRU} cache instance
 * created for each decorated method. The cache is a simple key-value store
 * which evicts the least recently used entry when it reaches maxSize.
 *
 * ### Structure
 *
 * To prevent memory leaks, all cache instances are contained within two layers
 * of ephemeral weak maps: an outer WeakMap that is keyed by the class itself,
 * which holds an inner WeakMap keyed by the `this` context (the class instance
 * or constructor, for static methods). Inside the inner map, a `Map` is used
 * to associate LRU instances with the property keys of the decorated methods.
 *
 * This structure ensures that no LRU cache can outlive the class that it is
 * associated with, effectively tying it to the lifetime of the class instance
 * and that of the class itself. This means cache entries can be automatically
 * garbage collected once the class is no longer reachable by the program.
 *
 * ### Key Generation
 *
 * Keys are generated by serializing the method's arguments with `options.key`,
 * which uses `JSON.stringify` by default. This can be overridden by passing a
 * custom key generation function.
 *
 * #### Keygen Determinism
 *
 * When providing a custom key generator, always ensure it is deterministic and
 * capable of serializing the argument types that the decorated method expects.
 * This is crucial for cache hit detection and key matching.
 *
 * ## Options
 *
 * The decorator accepts an options bag with the following properties:
 *
 * - `key`: Custom key generator (default: {@link defaultKey})
 * - `maxSize`: Maximum number of entries to cache (default: `128`)
 * - `overrides`: Custom API overrides (for testing and advanced use-cases)
 * - `transform`: Post-processing function for cached values.
 * - `ttl`: Optional time-to-live in milliseconds for each cache entry.
 * - `eviction`: Eviction strategy to control when expired entries are removed.
 *
 * @example Real-world LRU-cached fetch API with TTL and custom keygen:
 * ```ts no-eval
 * import { lru } from "@decorators/lru";
 *
 * class FetchService {
 *   constructor(protected init?: RequestInit) {}
 *
 *   @lru({
 *     key: (url) => url.toString(),
 *     transform: async (res) => (await res).clone(),
 *     ttl: 5_000, // cache entries expire after 5 seconds
 *   })
 *   fetch(url: string | URL): Promise<Response> {
 *     return globalThis.fetch(url, this.init);
 *   }
 * }
 *
 * // ensuring our fetch service is cached and TTL'd as expected
 * const f = new FetchService({ headers: { "accept": "application/json" } });
 *
 * // first fetch is uncached and takes longer
 * let a = performance.now();
 * await (await f.fetch("https://jsonplaceholder.typicode.com/posts/6")).json();
 * console.log(performance.now() - a, "ms"); // ~300ms (uncached)
 *
 * // subsequent fetch is cached and faster
 * a = performance.now();
 * await (await f.fetch("https://jsonplaceholder.typicode.com/posts/6")).json();
 * console.log(performance.now() - a, "ms"); // ~1.8ms (cached)
 *
 * // ... a few seconds later (5s TTL) ...
 * a = performance.now();
 * await (await f.fetch("https://jsonplaceholder.typicode.com/posts/6")).json();
 * console.log(performance.now() - a, "ms"); // ~200ms (re-cached)
 * ```
 * @module decorator
 */
import {
  type ClassMethod,
  type ClassMethodDecorator,
  defaultKey,
} from "./_internal.ts";
import type { EvictionStrategy, Options } from "./options.ts";
import defaults from "./overrides.ts";
import type {
  CacheEntry,
  CacheKey,
  ExtendedCacheEntry,
  MapLike,
  MapLikeConstructor,
} from "./types.ts";

lru.defaultOptions = {
  maxSize: 128,
  key: defaultKey,
  transform: (value) => value,
  ttl: 0,
  eviction: "passive" as EvictionStrategy,
  overrides: { ...defaults },
} satisfies Options<object>;

/**
 * Stage 3 decorator factory that caches the result of a method call using a
 * Least Recently Used (LRU) caching strategy, with optional TTL support and
 * configurable eviction strategies.
 *
 * See the module-level documentation for in-depth examples and usage patterns.
 *
 * @template {object} This The type of the `this` context at runtime. If the
 * method is static, this is the class constructor; otherwise, this will be the
 * type of the class instance.
 * @template {ClassMethod<This,Args,Return>} Value The target call signature.
 * @template {readonly any[]} Args The method's argument signature, as a tuple.
 * @template [Return] The method's return type.
 * @param [options] Configuration options for the cache.
 * @returns A method decorator.
 *
 * @example LRU-cached Fibonacci method with TTL and active eviction:
 * ```ts ignore
 * import { lru } from "@decorators/lru";
 *
 * class Calculator {
 *   @lru({ maxSize: 64, ttl: 1000, eviction: "active" })
 *   fib(n = 2): number {
 *     if (n < 2) return n;
 *     return this.fib(n - 1) + this.fib(n - 2);
 *   }
 * }
 *
 * const calc = new Calculator();
 * console.log(calc.fib(8)); // computed and cached
 * ```
 * @category Decorators
 * @tags LRU, cache
 */
export function lru<
  This extends object,
  Value extends ClassMethod<This, Args, Return>,
  Args extends readonly any[] = Parameters<Value>,
  Return = ReturnType<Value>,
>(
  options?: Options<This, NoInfer<Args>, Return>,
): ClassMethodDecorator<This, Value> {
  const {
    maxSize = 128,
    key: getKey = defaultKey,
    overrides = {},
    eviction = "passive",
    // onClear = () => {},
    onEvict = () => {},
    onRefresh = () => {},
    onHit = () => {},
    onMiss = () => {},
    inspect = () => {},
    prepare = (value) => value,
    transform = (value) => value,
    ttl = 0,
    // shouldEvict = ({ age, expiresAt = 0 }) => (expiresAt && age > expiresAt),
  } = (options ?? {}) as Options<This, Args, Return>;

  const keygen = getKey as (this: This, ...args: Args) => CacheKey;

  const {
    Date = defaults.Date,
    LRU = defaults.LRU,
    Map = defaults.Map as MapLikeConstructor,
    WeakMap = defaults.WeakMap as MapLikeConstructor,
    storage = defaults.storage,
  } = { ...defaults, ...overrides };

  if (typeof keygen !== "function") {
    throw new TypeError("Function expected: options.key");
  }

  return function (method, context) {
    let caches:
      | MapLike<string | symbol, MapLike<string, CacheEntry<Return>>>
      | undefined;
    let cache: MapLike<string, CacheEntry<Return>> | undefined;

    // Initialize the cache storage for the current context.
    context.addInitializer(getCache);

    const fn = function (this: This, ...args: Args): Return {
      const key = keygen.call(this, ...args);
      cache = getCache.call(this); // in case of a context change

      let evicted = false;

      let entry = cache.get(key);
      if (entry?.expiresAt && Date.now() > entry.expiresAt) {
        evicted = true;
        // Entry has expired, remove it from the cache.
        onEvict.call(this, entry.value, key, entry);
        // If the entry has expired, clear any active timer.
        if (entry.timer) clearTimeout(entry.timer);
        cache.delete(key);
        entry = undefined;
      }

      if (!entry) {
        // cache miss
        if (!evicted) onMiss.call(this, key);
        const value = method.call(this, ...args);
        entry = createEntry.call(this, { key, value });
      } else if (!evicted) {
        // cache hit
        onHit.call(this, entry.value, key, entry);
        if (entry.timer) {
          // Clear any existing timer before setting a new one.
          clearTimeout(entry.timer);
        }
      }

      // cache hit
      if (evicted) onRefresh.call(this, entry.value, key, entry);

      // if a pre-processor is provided, apply it to the value.
      const originalValue = entry.value;
      const prepared = prepare.call(this, entry.value, key, entry);
      entry.value = prepared ?? originalValue;

      if (ttl > 0) setCacheTimer.call(this, entry, ttl);

      // update recency: remove and reinsert
      cache.delete(key);
      cache.set(key, entry);

      const finalizedValue = transform(entry.value, key, entry);

      inspect.call(this, {
        ...entry,
        ttl,
        originalValue,
        finalizedValue,
        cache,
      });

      return finalizedValue;
    } as Value;

    // let's obscure the fact we've memoized this method
    return Object.defineProperties(fn, {
      ...Object.getOwnPropertyDescriptors(method),
      toString: {
        value: method.toString.bind(method),
        configurable: true,
      },
    });

    function createEntry<
      T extends
        & Omit<CacheEntry<Return>, "age" | "createdAt" | "expiresAt">
        & Partial<ExtendedCacheEntry<Return>>,
    >(
      this: This | void,
      entry: T,
    ): T & CacheEntry<Return> {
      const cache2 = cache ?? (
        typeof this === "undefined" ? undefined : getCache.call(this)
      );
      const createdAt = entry.createdAt ?? Date.now();
      const expiresAt = entry.expiresAt ?? 0;

      return {
        cache: cache2,
        ...entry,
        createdAt,
        expiresAt,
        get age() {
          return Date.now() - (this.createdAt ??= createdAt);
        },
      };
    }

    function setCacheTimer(
      this: This,
      entry: CacheEntry<Return>,
      ttl: number,
    ) {
      const newExpiration = Date.now() + ttl;
      entry.expiresAt = newExpiration;
      if (eviction === "active") {
        // Clear any existing timer before setting a new one.
        entry.timer && clearTimeout(entry.timer);
        // Set a timer to remove the entry after TTL expires.
        entry.timer = setTimeout(() => {
          onEvict.call(this, entry.value, entry.key, entry);

          // TODO(nberlette): is this line really needed?
          cache ??= getCache.call(this);
          cache.delete(entry.key);

          // cleanup the entry and dipose of the timer handle.
          clearTimeout(entry.timer);

          entry.timer = entry.expiresAt = entry.value = undefined!;
        }, ttl);
      }
    }

    // hoisted helper functions
    function getCacheStorage(key: object) {
      return storage.get(key) ?? storage.set(key, new WeakMap()).get(key)!;
    }

    function getCache(this: This, p = context.name) {
      // Prefer using the decorator metadata object as a key.
      let key: object = context.metadata;
      // Fallback to the constructor if metadata is not supported.
      key ??= context.static ? this : this.constructor;
      // Retrieve/initialize the outer weak map for the class constructor.
      const outer = getCacheStorage(key);
      if (!(caches ??= outer.get(this))) outer.set(this, caches = new Map());
      // Retrieve/initialize the actual LRU cache for this method.
      if (!(cache ??= caches.get(p))) caches.set(p, cache = new LRU(maxSize));
      return cache;
    }
  };
}

export default lru;
